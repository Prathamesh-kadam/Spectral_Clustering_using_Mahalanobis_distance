# -*- coding: utf-8 -*-
"""17_africa_sites

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N2CHpuixuuya1VP-48DhEZzxi2LncmlG
"""

import os
import numpy as np
import pandas as pd
import math
from sklearn.cluster import KMeans
import matplotlib.cm as cm
import matplotlib.pyplot as plt

# Path to the folder containing the datasets
input_directory = "/content/drive/MyDrive/Results-Africa/"
output_directory = "/content/drive/MyDrive/Calculated_data17/"

# Create the output directory if it does not exist
os.makedirs(output_directory, exist_ok=True)

# List all CSV files in the input directory
datasets = [f for f in os.listdir(input_directory) if f.endswith('.csv')]

# Define the function for processing each dataset
def process_dataset(file_path, dataset_name):
    # Load the data
    try:
        DATA = pd.read_csv(file_path, delimiter=",")
        DATA.columns = DATA.columns.str.strip()
        DATA.replace([np.inf, -np.inf], np.nan, inplace=True)
        # Optionally, fill NaNs with a specific value or method
        DATA.fillna(DATA.mean(), inplace=True)
        #DATA.dropna(inplace=True)
        DATA.reset_index(inplace=True, drop=True)
        print(f"Loaded dataset {dataset_name} with shape: {DATA.shape}")

        # Ensure data is not empty after preprocessing
        if DATA.empty:
            print(f"Warning: Dataset {dataset_name} is empty after preprocessing. Skipping.")
            return
    except Exception as e:
        print(f"Error loading dataset {dataset_name}: {e}")
        return

    # Define the columns you need
    selected_columns = [
        "AOD_F_440",
        "SSA440",
        "EAE_440_870",
        "AAE_440_870",
        "RI_R_440"
    ]

    # Check if all required columns are present
    missing_columns = [col for col in selected_columns if col not in DATA.columns]
    if missing_columns:
        print(f"Warning: Missing columns {missing_columns} in dataset {dataset_name}. Skipping this dataset.")
        return  # Skip this dataset if any required columns are missing

    # Select specific columns
    DATA = DATA[selected_columns]

    # Convert all columns to float64
    for col in DATA.columns:
        DATA[col] = DATA[col].astype(float)

    # Calculate necessary matrices and save them
    N = len(DATA)
    P = len(DATA.columns)

    corr_matrix_R = calculate_corr_matrix(DATA, dataset_name)
    WMD_matrix = calculate_WMD(DATA, dataset_name, corr_matrix_R)
    similarity_matrix_W = calculate_similarity_matrix(DATA, dataset_name, WMD_matrix, k=5)
    diagonal_matrix_D = calculate_diagonal_matrix(dataset_name, similarity_matrix_W)
    regularized_laplacian_matrix_L = calculate_regularized_laplacian_matrix(dataset_name, similarity_matrix_W, diagonal_matrix_D)

    eigenvalues_L, eigenvectors_L = np.linalg.eigh(regularized_laplacian_matrix_L)
    eigenvectors_P = calculate_k_smallest_eigenvectors(dataset_name, eigenvalues_L, eigenvectors_L, k=5)


def calculate_corr_matrix(DATA, dataset_name):
    corr_matrix = np.corrcoef(DATA.values, rowvar=False)
    corr_matrix_data = pd.DataFrame(corr_matrix)

    output_path = os.path.join(output_directory, dataset_name)
    os.makedirs(output_path, exist_ok=True)

    corr_matrix_data.to_csv(os.path.join(output_path, "corr_matrix_R.csv"))

    return corr_matrix


def calculate_WMD(DATA, dataset_name, corr_matrix):
    N = len(DATA)
    WMD = np.zeros((N, N), dtype=np.float64)

    for i in range(N):
        for j in range(i, N):
            X = np.array(DATA.loc[i])
            Y = np.array(DATA.loc[j])
            ans = np.dot(corr_matrix, X - Y)
            ans = np.dot(np.transpose(X - Y), ans)
            WMD[i][j] = math.sqrt(ans)

    # Make WMD symmetric
    WMD = WMD + WMD.T - np.diag(WMD.diagonal())

    WMD_data = pd.DataFrame(WMD)
    output_path = os.path.join(output_directory, dataset_name)
    WMD_data.to_csv(os.path.join(output_path, "WMD_matrix.csv"))

    return WMD


def calculate_similarity_matrix(DATA, dataset_name, WMD, k=3, z=3):
    N = len(DATA)
    similarity_matrix = np.zeros((N, N), dtype=np.float64)
    k_nn = np.zeros((N, N), dtype=np.bool_)

    for i in range(N):
        neighbor_WMD = WMD[i].copy()
        neighbor_WMD[i] = math.inf
        for j in range(k):
            min_WMD = min(neighbor_WMD)
            idx_neighbor = np.where(neighbor_WMD == min_WMD)[0][0]
            neighbor_WMD[idx_neighbor] = math.inf
            k_nn[i][idx_neighbor] = 1

    knn_data = pd.DataFrame(k_nn)
    output_path = os.path.join(output_directory, dataset_name)
    knn_data.to_csv(os.path.join(output_path, "k_nn_data.csv"))

    for i in range(N):
        for j in range(i, N):
            if k_nn[i][j] and k_nn[j][i]:
                coeff = (WMD[i][j] ** 2) / (2 * z)
                similarity_matrix[i][j] = math.exp(-coeff)
                similarity_matrix[j][i] = similarity_matrix[i][j]

    similarity_matrix_data = pd.DataFrame(similarity_matrix)
    similarity_matrix_data.to_csv(os.path.join(output_path, "similarity_matrix.csv"))

    return similarity_matrix


def calculate_diagonal_matrix(dataset_name, W):
    D = np.diag(W.sum(axis=1)) + np.eye(W.shape[0]) * 1e-5
    D_data = pd.DataFrame(D)

    output_path = os.path.join(output_directory, dataset_name)
    D_data.to_csv(os.path.join(output_path, "diagonal_matrix.csv"))

    return D


def calculate_regularized_laplacian_matrix(dataset_name, W, D):
    L = D - W
    D_inv_sqrt = np.linalg.inv(np.sqrt(D))
    regularized_L = D_inv_sqrt @ L @ D_inv_sqrt + 1e-5 * np.eye(D.shape[0])

    regularized_L_data = pd.DataFrame(regularized_L)
    output_path = os.path.join(output_directory, dataset_name)
    regularized_L_data.to_csv(os.path.join(output_path, "regularized_laplacian_matrix.csv"))

    return regularized_L


def calculate_k_smallest_eigenvectors(dataset_name, eigenvalues, eigenvectors, k):
    k_smallest_eig_V = eigenvectors[:, :k]
    k_smallest_eig_V /= np.linalg.norm(k_smallest_eig_V, axis=1, keepdims=True)
    np.nan_to_num(k_smallest_eig_V, copy=False, nan=(1 / np.sqrt(k)))

    eig_V_data = pd.DataFrame(k_smallest_eig_V)
    output_path = os.path.join(output_directory, dataset_name)
    eig_V_data.to_csv(os.path.join(output_path, f"{k}_eigenvectors.csv"))

    return k_smallest_eig_V

# Iterate over each dataset and process
for dataset in datasets:
    dataset_name = dataset.replace(".csv", "")
    file_path = os.path.join(input_directory, dataset)
    process_dataset(file_path, dataset_name)

import pandas as pd

# Assuming 'df' is your DataFrame
DATA = pd.read_csv("/content/drive/MyDrive/Results-Africa/Banizoumbou-inver_op.csv", delimiter=",")
data_types = DATA.dtypes
print("Data types of all columns:")
print(data_types)

import os
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans

num_clusters = 4
eigenvector_filename = "5_eigenvectors.csv"  # This is the name of the eigenvector file in each folder

def perform_clustering_and_save(file_path, num_clusters=4):
    # Read the eigenvector matrix
    X = pd.read_csv(file_path, delimiter=",")

    # Initialize KMeans
    kmeans = KMeans(
        n_clusters=num_clusters, init="k-means++", random_state=2, max_iter=300, tol=1e-1
    )
    kmeans.fit(X)

    # Get the predicted cluster labels
    pred = kmeans.labels_

    # Create a DataFrame for the cluster predictions
    cluster_prediction_data = pd.DataFrame(pred, columns=["Labels"])

    # Determine the output directory (same as the input file's directory)
    output_directory = os.path.dirname(file_path)

    # Save the cluster prediction results
    filename = "cluster_prediction.csv"
    full_path = os.path.join(output_directory, filename)
    np.savetxt(full_path, pred, delimiter=',', fmt='%f')

    print(f"Cluster prediction matrix saved at: {full_path}")

def process_all_datasets(root_directory, eigenvector_filename, num_clusters=4):
    # Iterate over all subdirectories in the root directory
    for dataset_folder in os.listdir(root_directory):
        dataset_folder_path = os.path.join(root_directory, dataset_folder)

        if os.path.isdir(dataset_folder_path):
            # Look for the eigenvector file in the current dataset folder
            eigenvector_file_path = os.path.join(dataset_folder_path, eigenvector_filename)

            # Debugging statement to print the expected file path
            print(f"Looking for file at: {eigenvector_file_path}")

            if os.path.exists(eigenvector_file_path):
                print(f"Processing dataset folder: {dataset_folder}")
                perform_clustering_and_save(eigenvector_file_path, num_clusters)
            else:
                print(f"Eigenvector file not found in: {dataset_folder}. Skipping this folder.")

# Specify the root directory where all dataset folders are located
root_directory = "/content/drive/MyDrive/Calculated_data17"

# Process all datasets
process_all_datasets(root_directory, eigenvector_filename, num_clusters)

#DAKAR

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances_argmin_min

# Load the data
original_data = pd.read_csv("/content/drive/MyDrive/Results-Africa/Bujumbura-inver_op.csv", delimiter=",")

# Define the list of feature pairs
feature_pairs = [
    ('EAE_440_870', 'SSA440'),
    ('EAE_440_870', 'AOD_F_440'),
    ('EAE_440_870', 'AAE_440_870'),
    ('EAE_440_870', 'DPR_440'),
    ('EAE_440_870', 'AST440'),
    ('EAE_440_870', 'RI_R_440')
]

# Create a 4x2 grid of subplots
fig, axes = plt.subplots(3, 2, figsize=(16, 16))
axes = axes.flatten()

# Define a colormap for consistent colors
colors = plt.cm.viridis(np.linspace(0, 1, 4))
cluster_predictions=('/content/drive/MyDrive/Calculated_data17/Bujumbura-inver_op/cluster_prediction.csv')

# Function to calculate cluster percentages
def calculate_cluster_percentages(cluster_predictions):
    unique_clusters, counts = np.unique(cluster_predictions, return_counts=True)
    total_points = len(cluster_predictions)
    percentages = (counts / total_points) * 100
    return dict(zip(unique_clusters, percentages))

# Function to handle empty clusters
def handle_empty_clusters(kmeans, X, num_clusters):
    labels = kmeans.labels_
    centroids = kmeans.cluster_centers_

    unique_labels = np.unique(labels)
    if len(unique_labels) < num_clusters:
        # If there are empty clusters, reassign to nearest cluster
        distances = pairwise_distances_argmin_min(centroids, centroids)
        for i in range(num_clusters):
            if i not in unique_labels:
                # Find the closest existing centroid
                closest_centroid = np.argmin(distances[0])
                # Assign all points to the nearest existing centroid
                labels[labels == i] = closest_centroid
        kmeans.labels_ = labels
        centroids = kmeans.cluster_centers_
    return centroids, labels

# Function to reorder clusters based on percentage
def reorder_clusters(centroids, labels):
    percentages = calculate_cluster_percentages(labels)
    sorted_clusters = sorted(percentages.items(), key=lambda x: -x[1])  # Sort by percentage, descending
    cluster_order = [cluster for cluster, _ in sorted_clusters]

    # Create a mapping from old label to new label
    label_mapping = {old_label: new_label for new_label, old_label in enumerate(cluster_order)}

    # Reassign labels based on the new order
    new_labels = np.array([label_mapping[label] for label in labels])
    new_centroids = centroids[cluster_order]

    return new_centroids, new_labels

# Perform clustering on each feature pair
for i, (feature_x, feature_y) in enumerate(feature_pairs):
    # Ensure numeric data
    original_data[feature_x] = pd.to_numeric(original_data[feature_x], errors='coerce')
    original_data[feature_y] = pd.to_numeric(original_data[feature_y], errors='coerce')

    # Extract features and drop rows with non-numeric values
    X = original_data[[feature_x, feature_y]].dropna()

    # Perform KMeans clustering on the selected features
    num_clusters = 4  # Adjust the number of clusters as needed
    kmeans = KMeans(n_clusters=num_clusters, random_state=2)
    kmeans.fit(X)

    # Handle empty clusters
    centroids, labels = handle_empty_clusters(kmeans, X, num_clusters)

    # Reorder clusters based on percentage
    centroids, labels = reorder_clusters(centroids, labels)

    # Calculate cluster percentages
    cluster_percentages = calculate_cluster_percentages(labels)

    # Visualization with consistent colors
    scatter = axes[i].scatter(X[feature_x], X[feature_y], c=labels, s=6, cmap='viridis')
    axes[i].scatter(centroids[:, 0], centroids[:, 1], marker='x', s=100, c='red', label='Centroids')

    # Annotate centroids with values and percentages
    for j, (cx, cy) in enumerate(centroids):
        percentage = cluster_percentages.get(j, 0)
        axes[i].annotate(f'({cx:.2f}, {cy:.2f})\n({percentage:.1f}%)', (cx, cy), textcoords='offset points', xytext=(5,5), ha='center')

    # Improve the readability of axis labels
    axes[i].tick_params(axis='both', labelsize=12)

    # Set axis limits for better visibility
    axes[i].set_xlim(X[feature_x].min() - 0.1, X[feature_x].max() + 0.1)
    axes[i].set_ylim(X[feature_y].min() - 0.1, X[feature_y].max() + 0.1)

    # Set title and labels
    axes[i].set_title(f'Clustering Results for {feature_x} vs {feature_y}')
    axes[i].set_xlabel(feature_x)
    axes[i].set_ylabel(feature_y)

    # Create a legend with percentages
    legend_labels = [f'Cluster {label} ({cluster_percentages.get(label, 0):.1f}%)' for label in range(num_clusters)]
    handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[label], markersize=10) for label in range(num_clusters)]
    axes[i].legend(handles, legend_labels, loc='upper right')

    axes[i].grid(True)

        # Print centroid values and percentages
    print(f"\nCentroid Values and Percentages for {feature_x} vs {feature_y}:")
    for j, (cx, cy) in enumerate(centroids):
        percentage = cluster_percentages.get(j, 0)
        print(f"Cluster {j}: Centroid ({cx:.2f}, {cy:.2f}) - {percentage:.1f}% of data points")

plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances_argmin_min
import os

# Define your main dataset and cluster label paths
results_path = "/content/drive/MyDrive/Results-Africa/"
calculated_data_path = "/content/drive/MyDrive/Calculated_data17/"

# Define the list of feature pairs
feature_pairs = [
    ('EAE_440_870', 'SSA440'),
    ('EAE_440_870', 'AOD_F_440'),
    ('EAE_440_870', 'AAE_440_870'),
    # ('EAE_440_870', 'DPR_440'),
    # ('EAE_440_870', 'AST440'),
    ('EAE_440_870', 'RI_R_440')
]

# Function to calculate cluster percentages
def calculate_cluster_percentages(labels):
    unique_clusters, counts = np.unique(labels, return_counts=True)
    total_points = len(labels)
    percentages = (counts / total_points) * 100
    return dict(zip(unique_clusters, percentages))

# Function to handle empty clusters
def handle_empty_clusters(kmeans, X, num_clusters):
    labels = kmeans.labels_
    centroids = kmeans.cluster_centers_

    unique_labels = np.unique(labels)
    if len(unique_labels) < num_clusters:
        for i in range(num_clusters):
            if i not in unique_labels:
                # Find the closest existing centroid
                closest_centroid = np.argmin(pairwise_distances_argmin_min(centroids, centroids)[0])
                # Assign all points to the nearest existing centroid
                labels[labels == i] = closest_centroid
        kmeans.labels_ = labels
        centroids = kmeans.cluster_centers_
    return centroids, labels

# Function to reorder clusters based on centroid x-values
def reorder_clusters_by_position(centroids, labels):
    # Get centroid x-values and sort by them
    x_values = centroids[:, 0]
    sorted_indices = np.argsort(x_values)

    # Create a mapping from old cluster labels to new sorted labels
    label_mapping = {old_label: new_label for new_label, old_label in enumerate(sorted_indices)}

    # Reassign labels based on the new mapping
    new_labels = np.array([label_mapping[label] for label in labels])
    new_centroids = centroids[sorted_indices]

    return new_centroids, new_labels

# Function to plot and save clustering results
def plot_clustering_results(X, feature_x, feature_y, centroids, labels, cluster_percentages, ax):
    scatter = ax.scatter(X[feature_x], X[feature_y], c=labels, s=6, cmap='viridis', vmin=0, vmax=3)
    ax.scatter(centroids[:, 0], centroids[:, 1], marker='x', s=100, c='red', label='Centroids')

    for j, (cx, cy) in enumerate(centroids):
        percentage = cluster_percentages.get(j, 0)
        ax.annotate(f'({cx:.2f}, {cy:.2f})\n({percentage:.1f}%)', (cx, cy), textcoords='offset points', xytext=(5, 5), ha='center')

    # Create legend with sorted clusters
    legend_labels = [f'Cluster {label} ({cluster_percentages.get(label, 0):.1f}%)' for label in range(len(centroids))]
    handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=plt.cm.viridis(label/len(centroids)), markersize=10) for label in range(len(centroids))]
    ax.legend(handles, legend_labels, loc='upper right')

    ax.set_xlabel(feature_x)
    ax.set_ylabel(feature_y)
    ax.set_title(f'{feature_x} vs {feature_y}')
    ax.grid(True)

# Loop through each result file and perform clustering
for file_name in os.listdir(results_path):
    if file_name.endswith(".csv"):
        dataset_name = file_name.replace(".csv", "")

        # Construct file paths
        data_file_path = os.path.join(results_path, file_name)
        cluster_label_file_path = os.path.join(calculated_data_path, dataset_name, "cluster_prediction.csv")

        try:
            # Load the data
            original_data = pd.read_csv(data_file_path)

            # Check if the necessary columns are in the dataset
            missing_features = [feature for pair in feature_pairs for feature in pair if feature not in original_data.columns]
            if missing_features:
                print(f"Skipping {dataset_name}: Missing columns {missing_features}")
                continue

            # Set up the figure and axes for subplots
            fig, axes = plt.subplots(2, 2, figsize=(16, 16))
            axes = axes.flatten()

            # Perform clustering on each feature pair and plot results
            for i, (feature_x, feature_y) in enumerate(feature_pairs):
                if i >= len(axes):
                    break  # Prevent index error if more feature pairs than subplots

                # Ensure numeric data
                original_data[feature_x] = pd.to_numeric(original_data[feature_x], errors='coerce')
                original_data[feature_y] = pd.to_numeric(original_data[feature_y], errors='coerce')

                # Extract features and drop rows with non-numeric values
                X = original_data[[feature_x, feature_y]].dropna()

                if X.empty:
                    print(f"Skipping clustering for {dataset_name}: Feature pair {feature_x} vs {feature_y} has no valid data.")
                    continue

                # Perform KMeans clustering on the selected features
                num_clusters = 4
                kmeans = KMeans(n_clusters=num_clusters, random_state=2)
                kmeans.fit(X)

                # Handle empty clusters
                centroids, labels = handle_empty_clusters(kmeans, X, num_clusters)

                # Reorder clusters based on centroid positions
                centroids, labels = reorder_clusters_by_position(centroids, labels)

                # Calculate cluster percentages
                cluster_percentages = calculate_cluster_percentages(labels)

                # Plot clustering results
                plot_clustering_results(X, feature_x, feature_y, centroids, labels, cluster_percentages, axes[i])

            # Save the figure
            plot_path = os.path.join(calculated_data_path, dataset_name, "clustering_results.png")
            plt.tight_layout()
            plt.savefig(plot_path)
            plt.close()
            # Print the path where the image is saved
            print(f"Clustering results for {dataset_name} saved to: {plot_path}")
            # Save the new cluster labels
            cluster_labels_df = pd.DataFrame(labels, columns=['Cluster_Label'])
            cluster_labels_df.to_csv(cluster_label_file_path, index=False)

        except Exception as e:
            print(f"Error processing {dataset_name}: {e}")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances_argmin_min
import os

# Define your main dataset and cluster label paths
results_path = "/content/drive/MyDrive/Results-Africa/"
calculated_data_path = "/content/drive/MyDrive/Calculated_data17/"

# Define the list of feature pairs
feature_pairs = [
    ('EAE_440_870', 'SSA440'),
    ('EAE_440_870', 'AOD_F_440'),
    ('EAE_440_870', 'AAE_440_870'),
    ('EAE_440_870', 'RI_R_440')
]

# Function to calculate cluster percentages
def calculate_cluster_percentages(labels):
    unique_clusters, counts = np.unique(labels, return_counts=True)
    total_points = len(labels)
    percentages = (counts / total_points) * 100
    return dict(zip(unique_clusters, percentages))

# Function to handle empty clusters
def handle_empty_clusters(kmeans, X, num_clusters):
    labels = kmeans.labels_
    centroids = kmeans.cluster_centers_

    unique_labels = np.unique(labels)
    if len(unique_labels) < num_clusters:
        for i in range(num_clusters):
            if i not in unique_labels:
                closest_centroid = np.argmin(pairwise_distances_argmin_min(centroids, centroids)[0])
                labels[labels == i] = closest_centroid
        kmeans.labels_ = labels
        centroids = kmeans.cluster_centers_
    return centroids, labels

# Function to reorder clusters based on centroid x-values
def reorder_clusters_by_position(centroids, labels):
    x_values = centroids[:, 0]
    sorted_indices = np.argsort(x_values)
    label_mapping = {old_label: new_label for new_label, old_label in enumerate(sorted_indices)}
    new_labels = np.array([label_mapping[label] for label in labels])
    new_centroids = centroids[sorted_indices]
    return new_centroids, new_labels

# Function to calculate coarse mode, mean, and median
def calculate_additional_stats(X):
    coarse_mode = X.mode().iloc[0].values if not X.empty else [np.nan] * X.shape[1]
    mean_values = X.mean().values if not X.empty else [np.nan] * X.shape[1]
    median_values = X.median().values if not X.empty else [np.nan] * X.shape[1]
    return coarse_mode, mean_values, median_values

# Function to plot and save clustering results
def plot_clustering_results(X, feature_x, feature_y, centroids, labels, cluster_percentages, ax):
    scatter = ax.scatter(X[feature_x], X[feature_y], c=labels, s=6, cmap='viridis', vmin=0, vmax=3)
    ax.scatter(centroids[:, 0], centroids[:, 1], marker='x', s=100, c='red', label='Centroids')
    for j, (cx, cy) in enumerate(centroids):
        percentage = cluster_percentages.get(j, 0)
        ax.annotate(f'({cx:.2f}, {cy:.2f})\n({percentage:.1f}%)', (cx, cy), textcoords='offset points', xytext=(5, 5), ha='center')
    legend_labels = [f'Cluster {label} ({cluster_percentages.get(label, 0):.1f}%)' for label in range(len(centroids))]
    handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=plt.cm.viridis(label/len(centroids)), markersize=10) for label in range(len(centroids))]
    ax.legend(handles, legend_labels, loc='upper right')
    ax.set_xlabel(feature_x)
    ax.set_ylabel(feature_y)
    ax.set_title(f'{feature_x} vs {feature_y}')
    ax.grid(True)

# Initialize a list to store centroid data for the CSV file
centroid_data = []

# Loop through each result file and perform clustering
for file_name in os.listdir(results_path):
    if file_name.endswith(".csv"):
        dataset_name = file_name.replace(".csv", "")

        # Construct file paths
        data_file_path = os.path.join(results_path, file_name)
        cluster_label_file_path = os.path.join(calculated_data_path, dataset_name, "cluster_prediction.csv")

        try:
            # Load the data
            original_data = pd.read_csv(data_file_path)

            # Check for missing features
            missing_features = [feature for pair in feature_pairs for feature in pair if feature not in original_data.columns]
            if missing_features:
                print(f"Skipping {dataset_name}: Missing columns {missing_features}")
                continue

            # Set up the figure for plotting
            fig, axes = plt.subplots(2, 2, figsize=(16, 16))
            axes = axes.flatten()

            # Process each feature pair
            for i, (feature_x, feature_y) in enumerate(feature_pairs):
                if i >= len(axes):
                    break

                original_data[feature_x] = pd.to_numeric(original_data[feature_x], errors='coerce')
                original_data[feature_y] = pd.to_numeric(original_data[feature_y], errors='coerce')

                X = original_data[[feature_x, feature_y]].dropna()

                if X.empty:
                    print(f"Skipping clustering for {dataset_name}: Feature pair {feature_x} vs {feature_y} has no valid data.")
                    continue

                num_clusters = 4
                kmeans = KMeans(n_clusters=num_clusters, random_state=2)
                kmeans.fit(X)

                centroids, labels = handle_empty_clusters(kmeans, X, num_clusters)
                centroids, labels = reorder_clusters_by_position(centroids, labels)
                cluster_percentages = calculate_cluster_percentages(labels)

                # Calculate coarse mode, mean, and median for each cluster
                coarse_mode, mean_values, median_values = calculate_additional_stats(X)

                # Store centroids and additional statistics
                for j, (cx, cy) in enumerate(centroids):
                    centroid_data.append({
                        'Dataset': dataset_name,
                        'Cluster': f'Cluster {j}',
                        'Feature': f'{feature_x} vs {feature_y}',
                        'Centroid X': cx,
                        'Centroid Y': cy,
                        'Percentage': cluster_percentages.get(j, 0),
                        'Coarse Mode X': coarse_mode[0],
                        'Coarse Mode Y': coarse_mode[1],
                        'Mean X': mean_values[0],
                        'Mean Y': mean_values[1],
                        'Median X': median_values[0],
                        'Median Y': median_values[1]
                    })

                plot_clustering_results(X, feature_x, feature_y, centroids, labels, cluster_percentages, axes[i])

            plot_path = os.path.join(calculated_data_path, dataset_name, "clustering_results.png")
            plt.tight_layout()
            plt.savefig(plot_path)
            plt.close()
            print(f"Clustering results for {dataset_name} saved to: {plot_path}")
            cluster_labels_df = pd.DataFrame(labels, columns=['Cluster_Label'])
            cluster_labels_df.to_csv(cluster_label_file_path, index=False)

        except Exception as e:
            print(f"Error processing {dataset_name}: {e}")

# Save centroid data to CSV in horizontal format
centroid_df = pd.DataFrame(centroid_data)
centroid_df_pivot = centroid_df.pivot_table(
    index=['Dataset', 'Cluster'],
    columns='Feature',
    values=['Centroid X', 'Centroid Y', 'Percentage', 'Coarse Mode X', 'Coarse Mode Y', 'Mean X', 'Mean Y', 'Median X', 'Median Y']
)
centroid_csv_path = os.path.join(calculated_data_path, "centroid_values_horizontal.csv")
centroid_df_pivot.to_csv(centroid_csv_path)
print(f"Centroid values saved to: {centroid_csv_path}")

from google.colab import drive
drive.mount('/content/drive')